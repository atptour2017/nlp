TF-IDF（Term Frequency-inverse Document Frequency）是一种针对关键词的统计分析方法，用于评估一个词对一个文件集或者一个语料库的重要程度。
一个词的重要程度跟它在文章中出现的次数成正比，跟它在语料库出现的次数成反比。这种计算方式能有效避免常用词对关键词的影响，提高了关键词与文章之间的相关性。

其中TF指的是某词在文章中出现的总次数，该指标通常会被归一化定义为TF=（某词在文档中出现的次数/文档的总词量），
这样可以防止结果偏向过长的文档（同一个词语在长文档里通常会具有比短文档更高的词频）。IDF逆向文档频率，包含某词语的文档越少，IDF值越大，
说明该词语具有很强的区分能力，IDF=loge（语料库中文档总数/包含该词的文档数+1），
+1的原因是避免分母为0。TFIDF=TFxIDF，TFIDF值越大表示该特征词对这个文本的重要性越大。

可以在Sklearn中调用TFIDFVectorizer库实现TF-IDF算法，并且可以通过stopwords参数来设置文档中的停用词（没有具体意义的词，如助词，语气词等），
使得停用词不纳入计算范围，提高算法的精确性。


什么是TF-IDF?

TF-IDF(term frequency–inverse document frequency)是一种用于信息检索与数据挖掘的常用加权技术，常用于挖掘文章中的关键词，而且算法简单高效，常被工业用于最开始的文本数据清洗。

TF-IDF有两层意思，一层是"词频"（Term Frequency，缩写为TF），另一层是"逆文档频率"（Inverse Document Frequency，缩写为IDF）。

假设我们现在有一片长文叫做《量化系统架构设计》词频高在文章中往往是停用词，“的”，“是”，“了”等，这些在文档中最常见但对结果毫无帮助、需要过滤掉的词，用TF可以统计到这些停用词并把它们过滤。
当高频词过滤后就只需考虑剩下的有实际意义的词。
但这样又会遇到了另一个问题，我们可能发现"量化"、"系统"、"架构"这三个词的出现次数一样多。这是不是意味着，作为关键词，它们的重要性是一样的？事实上系统应该在其他文章比较常见，所以在关键词排序上，
“量化”和“架构”应该排在“系统”前面，这个时候就需要IDF，IDF会给常见的词较小的权重，它的大小与一个词的常见程度成反比。

当有TF(词频)和IDF(逆文档频率)后，将这两个词相乘，就能得到一个词的TF-IDF的值。某个词在文章中的TF-IDF越大，那么一般而言这个词在这篇文章的重要性会越高，所以通过计算文章中各个词的TF-IDF，由
大到小排序，排在最前面的几个词，就是该文章的关键词。


优缺点

TF-IDF的优点是简单快速，而且容易理解。缺点是有时候用词频来衡量文章中的一个词的重要性不够全面，有时候重要的词出现的可能不够多，而且这种计算无法体现位置信息，无法体现词在上下文的重要性。如果要体现
词的上下文结构，那么你可能需要使用word2vec算法来支持。


https://zhuanlan.zhihu.com/p/97273457